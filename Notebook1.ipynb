{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bd072b8",
   "metadata": {},
   "source": [
    "# Notebook 1: Data pre-processing and management.\n",
    "\n",
    "This is the first notebook in the final project for Machine Learning. \n",
    "\n",
    "The objective of this preprocessing stage is to construct a structured dataset of fixed-size spectrograms derived from annotated seal calls. These spectrograms will serve as the input features for subsequent machine learning analyses aimed at discriminating between different call types. An additional “no-call” class is also generated to represent background audio segments without vocalisations.\n",
    "\n",
    "All preprocessing steps are implemented within a single Jupyter notebook to ensure transparency, reproducibility, and ease of inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390e77b8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e3c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html\n",
    "import matplotlib.pyplot as plt\n",
    "# https://docs.scipy.org/doc/scipy/tutorial/signal.html\n",
    "from scipy import signal\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.html\n",
    "from scipy.io import wavfile\n",
    "# https://numpy.org/doc/stable/\n",
    "import numpy as np\n",
    "# https://pandas.pydata.org/docs/\n",
    "import pandas as pd\n",
    "# https://matplotlib.org/stable/api/colors_api.html#matplotlib.colors.LogNorm\n",
    "from matplotlib.colors import LogNorm\n",
    "# to enable zooming in matplotlib in notebook\n",
    "#%matplotlib notebook \n",
    "#conda install -c conda-forge ipympl # or pip install ipympl #This is to allow zooming in the notebook\n",
    "# %matplotlib ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70d5dc8",
   "metadata": {},
   "source": [
    "Note the path to the data files. The data is not stored in the repository (as it is too large and also private to ATU's MFRC). Instead it is stored in the one drive on the desktop. \n",
    "Each recording consists of:\n",
    "A .wav audio file containing the continuous acoustic recording\n",
    "\n",
    "A corresponding annotation file (.Table.1.selections.txt) specifying call start time, call end time, frequency bounds, call type (e.g. Rupe A, Rupe B, Gutteral and Moan)\n",
    "\n",
    "These annotations were produced using acoustic analysis software and are treated as ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#File index\n",
    "file =  r'C:\\Users\\grace\\OneDrive\\Desktop\\GreySealData\\Rupes A and B\\5713.210825190002'  #from PPT at time 892-896 (Rupe B)\n",
    "#file = r'C:\\Users\\grace\\OneDrive\\Desktop\\GreySealData\\Guttural rupe\\\\5711.211013040024'\n",
    "#file = r'C:\\Users\\grace\\OneDrive\\Desktop\\GreySealData\\\\Rupes A and B\\\\5713.210825190002'\n",
    "#file = r'C:\\Users\\grace\\OneDrive\\Desktop\\GreySealData\\\\Moan\\\\5713.210902110002'  #from PPT at time 212 seconds\n",
    "\n",
    "#Read the 2 files\n",
    "sample_rate, samples = wavfile.read(file+'.wav')\n",
    "annot_file_path = file +'.Table.1.selections.txt'\n",
    "\n",
    "#Read the file into a DataFrame\n",
    "df = pd.read_csv(file +'.Table.1.selections.txt', sep='\\t')\n",
    "\n",
    "#Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "print(f'Sample rate: {sample_rate} Hz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b398520",
   "metadata": {},
   "source": [
    "## Spectogram Computation\n",
    "\n",
    "A short time Fourier transform-based spectogram is computed using parameters selected to balance time and frequency resolution while capturing the frequency range of seal vocalisations\n",
    "\n",
    "https://en.wikipedia.org/wiki/Short-time_Fourier_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ef01ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Spectrogram\n",
    "frequencies, times, spectrogram = signal.spectrogram(samples, sample_rate, nperseg=2456, nfft=4096, noverlap=1228, window='hann')       #Try nfft=8192 if computer permits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e231426",
   "metadata": {},
   "source": [
    "## Frequency band selection\n",
    "With the resultsing spectrogram, trim all the tiny values so that log scale displays correctly. Also, all of the relevant info is below 1KHz so trim the data to only display sub 1-KHz frequencies. Restricting the frequency range reduces dimensionality and removes irrelevant noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65123c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Spectrogram\n",
    "frequencies, times, spectrogram = signal.spectrogram(samples, sample_rate, nperseg=2456, nfft=4096, noverlap=1228, window='hann')       #Try nfft=8192 if computer permits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc39da8",
   "metadata": {},
   "source": [
    "## Visual Validation with Annotations.\n",
    "Define a function that will overlay the annotated rectangles onto the spectrogram (different colours for each call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0cd34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_annotations(ax, df, annotation_colors):\n",
    "    #Track labels to ensure they are added only once in the legend\n",
    "    added_labels = set()\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        start_time = row['Begin Time (s)']\n",
    "        end_time = row['End Time (s)']\n",
    "        low_freq = row['Low Freq (Hz)']\n",
    "        high_freq = row['High Freq (Hz)']\n",
    "        annotation = row['Annotation']\n",
    "\n",
    "        #Skip if the annotation is not in the defined colors\n",
    "        if annotation not in annotation_colors:\n",
    "            continue\n",
    "\n",
    "        #Draw rectangles\n",
    "        ax.add_patch(\n",
    "            plt.Rectangle(\n",
    "                (start_time, low_freq),  #Bottom Left corner\n",
    "                end_time - start_time,  #Width (time)\n",
    "                high_freq - low_freq,  #Height (frequency)\n",
    "                edgecolor=annotation_colors[annotation],\n",
    "                facecolor='none',\n",
    "                linewidth=2,\n",
    "                label=annotation if annotation not in added_labels else None  #Add label once\n",
    "            )\n",
    "        )\n",
    "        added_labels.add(annotation)  #Mark label as added\n",
    "\n",
    "    ax.legend(loc='upper right')     #Add legend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c3e3e0",
   "metadata": {},
   "source": [
    "A function below that updates the colormap when you zoom in on a particular region - so that the max and min values are always visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a3d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_colormap(event):\n",
    "    #Get current view limits\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    #Find indices corresponding to the current view limits\n",
    "    x_indices = np.where((times >= xlim[0]) & (times <= xlim[1]))[0]\n",
    "    y_indices = np.where((frequencies >= ylim[0]) & (frequencies <= ylim[1]))[0]\n",
    "\n",
    "    #Handle cases where no data is visible\n",
    "    if len(x_indices) == 0 or len(y_indices) == 0:\n",
    "        return\n",
    "\n",
    "    #Extract the visible data\n",
    "    data_visible = spectrogram[np.ix_(y_indices, x_indices)]\n",
    "    #data_visible = np.log(spectrogram)[np.ix_(y_indices, x_indices)]\n",
    "\n",
    "    #Compute new color limits\n",
    "    vmin = np.nanmin(data_visible)\n",
    "    vmax = np.nanmax(data_visible)\n",
    "\n",
    "    #Update the color limits of the pcolormesh\n",
    "    pc.set_clim(vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    #Update the colorbar to reflect the new color limits\n",
    "    cbar.update_normal(pc)\n",
    "\n",
    "    #Redraw the figure\n",
    "    plt.draw()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3128fca5",
   "metadata": {},
   "source": [
    "Plot the spectrogram and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27903df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define colors for annotations\n",
    "annotation_colors = {\n",
    "    \"Rupe A\": \"red\",\n",
    "    \"Rupe B\": \"green\",\n",
    "    \"Growl B\": \"yellow\",\n",
    "    \"Rupe C\" : \"purple\",\n",
    "    \"Moan\": \"pink\",\n",
    "    \"G rupe\" : \"blue\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5570ae0b",
   "metadata": {},
   "source": [
    "The following cell is the quality assurance/inspection tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b164c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pc = plt.pcolormesh(times, frequencies, spectrogram, norm=LogNorm(), cmap='Spectral_r')\n",
    "#pc = plt.pcolormesh(times, frequencies, np.log(spectrogram))\n",
    "cbar = plt.colorbar(pc)\n",
    "#plt.imshow(spectrogram)\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "\n",
    "#Get the current axes\n",
    "ax = plt.gca()\n",
    "overlay_annotations(ax, df, annotation_colors)\n",
    "\n",
    "#Connect the update function to the axes limit change events\n",
    "ax.callbacks.connect('xlim_changed', update_colormap)\n",
    "ax.callbacks.connect('ylim_changed', update_colormap)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ca4ce9",
   "metadata": {},
   "source": [
    "Extract out a portion of interest from the spectrogram. This is the manual Sub-Spectogram Extraction (Ground Truth Validation). It confirms shape, orientation and frequency coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5494932f",
   "metadata": {},
   "source": [
    "#Define time and frequency limits\n",
    "time_start, time_end = 891, 897  # Time range in seconds\n",
    "freq_start, freq_end = 20, 600  # Frequency range in Hz\n",
    "\n",
    "#Find indices for the time range\n",
    "time_indices = np.where((times >= time_start) & (times <= time_end))[0]\n",
    "\n",
    "#Find indices for the frequency range\n",
    "freq_indices = np.where((frequencies >= freq_start) & (frequencies <= freq_end))[0]\n",
    "\n",
    "#Extract the portion of the spectrogram\n",
    "spectrogram_sub = spectrogram[freq_indices][:, time_indices]\n",
    "frequencies_sub = frequencies[freq_indices]\n",
    "times_sub = times[time_indices]\n",
    "print(spectrogram_sub.shape)\n",
    "\n",
    "#Plot the original and sub-portion spectrograms\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.pcolormesh(times, frequencies, spectrogram, norm=LogNorm(), cmap='Spectral_r')\n",
    "plt.title('Original Spectrogram')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.colorbar(label='Power [dB]')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.pcolormesh(times_sub, frequencies_sub, spectrogram_sub, norm=LogNorm(), cmap='Spectral_r')\n",
    "plt.title('Extracted Portion of Spectrogram')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.colorbar(label='Power [dB]')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21afeec",
   "metadata": {},
   "source": [
    "The above cell confirms extraction is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dbb38c",
   "metadata": {},
   "source": [
    "Extract a single call and check the size of the spectrogram array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b1a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define time and frequency limits\n",
    "time_start, time_end = 892.5, 893.2  # Time range in seconds\n",
    "freq_start, freq_end = 20, 600  # Frequency range in Hz\n",
    "\n",
    "#Find indices for the time range\n",
    "time_indices = np.where((times >= time_start) & (times <= time_end))[0]\n",
    "\n",
    "#Find indices for the frequency range\n",
    "freq_indices = np.where((frequencies >= freq_start) & (frequencies <= freq_end))[0]\n",
    "\n",
    "#Extract the portion of the spectrogram\n",
    "spectrogram_sub = spectrogram[freq_indices][:, time_indices]\n",
    "frequencies_sub = frequencies[freq_indices]\n",
    "times_sub = times[time_indices]\n",
    "print(\"Spectrogram size: \", spectrogram_sub.shape)\n",
    "\n",
    "#Plot the original and sub-portion spectrograms\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.pcolormesh(times, frequencies, spectrogram, norm=LogNorm(), cmap='Spectral_r')\n",
    "plt.title('Original Spectrogram')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.colorbar(label='Power [dB]')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.pcolormesh(times_sub, frequencies_sub, spectrogram_sub, norm=LogNorm(), cmap='Spectral_r')\n",
    "plt.title('Extracted Portion of Spectrogram')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.colorbar(label='Power [dB]')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2ef391",
   "metadata": {},
   "source": [
    "## printing the Spectogram Array\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.spectrogram.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f2936",
   "metadata": {},
   "source": [
    "## Determining the baseline spectogram size.\n",
    "\n",
    "This is to ensure consistancy across all samples. All the spectograms must have the same dimentions. Identification of the longest call duration and the maximum frequency bin count after frequency trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba85c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duration'] = df['End Time (s)'] - df['Begin Time (s)']\n",
    "max_duration = df['duration'].max()\n",
    "max_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184be5cb",
   "metadata": {},
   "source": [
    "## Extracting Call-Centred Spectograms.\n",
    "\n",
    "Each call is extracted by centering in a fixed-length window on the midpoint of the annoted call. ie the call is in the middle of the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea6ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_call_spectrogram(center_time, duration):\n",
    "    start_sample = int((center_time - duration / 2) * sample_rate)\n",
    "    end_sample = int((center_time + duration / 2) * sample_rate)\n",
    "\n",
    "    segment = samples[start_sample:end_sample]\n",
    "\n",
    "    f, t, Sxx = signal.spectrogram(\n",
    "        segment,\n",
    "        fs=sample_rate,\n",
    "        nperseg=2456,\n",
    "        nfft=4096,\n",
    "        noverlap=1228,\n",
    "        window='hann'\n",
    "    )\n",
    "\n",
    "    Sxx[Sxx < 0.001] = 0.001\n",
    "    Sxx = Sxx[freq_indices, :]\n",
    "\n",
    "    # Log scaling for ML\n",
    "    return np.log10(Sxx + 1e-10)\n",
    "    return Sxx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d903b0",
   "metadata": {},
   "source": [
    "## Extracting and saving call Spectograms\n",
    "\n",
    "Saved as raw NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514721ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of files to process\n",
    "files = [\n",
    "    r'C:\\Users\\grace\\OneDrive\\Desktop\\GreySealData\\Rupes A and B\\5713.210825190002',\n",
    "    r'C:\\Users\\grace\\OneDrive\\Desktop\\GreySealData\\Guttural rupe\\5711.211013040024',\n",
    "    r'C:\\Users\\grace\\OneDrive\\Desktop\\GreySealData\\Rupes A and B\\5713.210825190002',\n",
    "    r'C:\\Users\\grace\\OneDrive\\Desktop\\GreySealData\\Moan\\5713.210902110002'\n",
    " ]\n",
    "\n",
    "# Initialize combined lists for spectrograms and metadata\n",
    "combined_spectrograms = []\n",
    "combined_metadata = []\n",
    "\n",
    "for file in files:\n",
    "    # Read the audio file and annotation file\n",
    "    sample_rate, samples = wavfile.read(file + '.wav')\n",
    "    df = pd.read_csv(file + '.Table.1.selections.txt', sep='\\t')\n",
    "\n",
    "    # Process each annotation in the file\n",
    "    for _, row in df.iterrows():\n",
    "        center_time = (row['Begin Time (s)'] + row['End Time (s)']) / 2\n",
    "        spec = extract_call_spectrogram(center_time, max_duration)\n",
    "\n",
    "        combined_spectrograms.append(spec)\n",
    "        combined_metadata.append({\n",
    "            \"call_type\": row['Annotation'],\n",
    "            \"center_time\": center_time,\n",
    "            \"duration\": max_duration\n",
    "        })\n",
    "\n",
    "# Save the combined results\n",
    "np.save(\"call_spectrograms.npy\", np.array(combined_spectrograms))\n",
    "pd.DataFrame(combined_metadata).to_csv(\"call_metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8795c8",
   "metadata": {},
   "source": [
    "## \"No Call\" spectogram.\n",
    "\n",
    "To provide a negative class, spectrograms are extracted from unannotated regions. These samples use the same duration and frequency range as call spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac48eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://numpy.org/doc/2.3/reference/generated/numpy.linspace.html\n",
    "no_call_times = np.linspace(\n",
    "    max_duration,\n",
    "    times[-1] - max_duration,\n",
    "    10\n",
    ")\n",
    "\n",
    "no_call_specs = []\n",
    "\n",
    "for t in no_call_times:\n",
    "    no_call_specs.append(extract_call_spectrogram(t, max_duration))\n",
    "\n",
    "np.save(\"no_call_spectrograms.npy\", np.array(no_call_specs))\n",
    "\n",
    "no_call_times = np.linspace(\n",
    "    max_duration,\n",
    "    times[-1] - max_duration,\n",
    "    10\n",
    ")\n",
    "\n",
    "no_call_specs = []\n",
    "\n",
    "for t in no_call_times:\n",
    "    no_call_specs.append(extract_call_spectrogram(t, max_duration))\n",
    "\n",
    "np.save(\"no_call_spectrograms.npy\", np.array(no_call_specs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e6159",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spectrogram_sub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb8e1f0",
   "metadata": {},
   "source": [
    "This is the 2D numpy array that the ML model will see. Each row = frequency bin and each column = time bin. The values are the spectral power (log-scaled via plotting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2869727b",
   "metadata": {},
   "source": [
    "## Log scale before saving for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/theoviel/spectrogram-generation\n",
    "spectrogram_sub_ml = np.log10(spectrogram_sub + 1e-10)\n",
    "print(spectrogram_sub_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f46f9a",
   "metadata": {},
   "source": [
    "Log scaled spectograms are used in Machine learning as input features in audio classification tasks, including music analysis, speech processing and bioacoustics, which is what we have here. Studies show that models trained on logarithmic or decibel-scaled spectograms consistently outperform those trained on linear-scaled representations.\n",
    "\n",
    "By applying this transformation prior to saving the data, the resulting 2D arrays become directly suitable for classical machine learning models (e.g., SVMs, random forests) as well as convolutional neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ef8f31",
   "metadata": {},
   "source": [
    "## Summary of this notebook\n",
    "\n",
    "This is the preprocessing pipeline. It converts large raw data from the <i>'Grey Seal Project'</i> into fixed sized spectograms while preserving the spectral and temporal consistency. It produces both call and no call examples and stores data in a format usable for Machine Learning ie numerical tensor form not as images. Call examples correspond to annotated grey seal vocalisations (e.g. Rupe A, Rupe B, Moan), as defined in the provided annotation files. No-call examples are extracted from unannotated regions of the recordings and represent background acoustic conditions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679f13c3",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
